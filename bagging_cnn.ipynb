{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa6m2XxrmMGQ0pC2lGeJaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frankfurtmacmoses/cnn_bagging/blob/main/bagging_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmROAJZXDwZQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73636584-576a-408d-c4b4-6134f79a8a8f",
        "id": "OVRTzxteSaox"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fxv9QnWqELoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from plotnine import ggplot, aes, geom_tile, geom_text, scale_fill_gradient, labs, theme_minimal\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKrRuIB6T_zz",
        "outputId": "4842979c-e4cf-4ba1-f33b-3512c9bc45c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100 = tf.keras.datasets.cifar100\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hca_g4MTUupQ",
        "outputId": "3bc45814-65d6-49ba-d486-bc98c2582295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Cv_H7hbEaPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()"
      ],
      "metadata": {
        "id": "gzy88WfgU57J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    'beaver', 'dolphin', 'otter', 'seal', 'whale',          # Aquatic mammals\n",
        "    'aquarium fish', 'flatfish', 'ray', 'shark', 'trout',   # Fish\n",
        "    'orchids', 'poppies', 'roses', 'sunflowers', 'tulips',  # Flowers\n",
        "    'bottles', 'bowls', 'cans', 'cups', 'plates',           # Food containers\n",
        "    'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers',  # Fruit and vegetables\n",
        "    'clock', 'computer keyboard', 'lamp', 'telephone', 'television',  # Household electrical devices\n",
        "    'bed', 'chair', 'couch', 'table', 'wardrobe',           # Household furniture\n",
        "    'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach',  # Insects\n",
        "    'bear', 'leopard', 'lion', 'tiger', 'wolf',             # Large carnivores\n",
        "    'bridge', 'castle', 'house', 'road', 'skyscraper',      # Large man-made outdoor things\n",
        "    'cloud', 'forest', 'mountain', 'plain', 'sea',          # Large natural outdoor scenes\n",
        "    'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo',  # Large omnivores and herbivores\n",
        "    'fox', 'porcupine', 'possum', 'raccoon', 'skunk',       # Medium-sized mammals\n",
        "    'crab', 'lobster', 'snail', 'spider', 'worm',           # Non-insect invertebrates\n",
        "    'baby', 'boy', 'girl', 'man', 'woman',                  # People\n",
        "    'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle',   # Reptiles\n",
        "    'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel',      # Small mammals\n",
        "    'maple', 'oak', 'palm', 'pine', 'willow',               # Trees\n",
        "    'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train',  # Vehicles 1\n",
        "    'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor'  # Vehicles 2\n",
        "]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(100,7))\n",
        "p = sns.countplot(y_train.flatten())\n",
        "p.set(xticklabels=classes)"
      ],
      "metadata": {
        "id": "nQxAFQdmVYcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32, 32, 3)\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
        "x_train=x_train / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
        "x_test=x_test / 255.0"
      ],
      "metadata": {
        "id": "09WWQ_Scbs-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cma_qtQIKjPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.one_hot(y_train.astype(np.int32), depth=100)\n",
        "y_test = tf.one_hot(y_test.astype(np.int32), depth=100)"
      ],
      "metadata": {
        "id": "oTMUugzldz1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define CNN Model Architecture\n"
      ],
      "metadata": {
        "id": "_yUlEdD5HORg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWDzI3TWFXFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', input_shape=x_train.shape[1:], activation='relu'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "   model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-06),\n",
        "            loss='categorical_crossentropy', metrics=['acc'])\n",
        "   return model\n"
      ],
      "metadata": {
        "id": "xqGO-JoKE0rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the model"
      ],
      "metadata": {
        "id": "Rsy87EhmHUYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_samples = 5       # Number of subsets\n",
        "subset_fraction = 0.25  # Fraction of population in each subset\n",
        "subset_size = int(len(x_train) * subset_fraction)\n",
        "\n",
        "num_models = 5\n",
        "models = []\n",
        "for _ in range(num_models):\n",
        "    model = create_cnn_model()\n",
        "    indices = np.random.choice(range(len(x_train)), size=subset_size, replace=True)\n",
        "    x_train_bootstrap = x_train[indices]\n",
        "    y_train_bootstrap = y_train[indices]\n",
        "\n",
        "    model.fit(x_train_bootstrap, y_train_bootstrap, epochs=10, batch_size=32, verbose=0)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "N-BqCm7IHYsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Prediction with bagging approach\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FWOX-phcdp6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predictions(models, x_test):\n",
        "    # Collect predictions from each model\n",
        "    num_classes = 100  # CIFAR-100 has 100 classes\n",
        "    predictions = np.zeros((x_test.shape[0], num_classes))\n",
        "\n",
        "    for model in models:\n",
        "        predictions += model.predict(x_test)\n",
        "\n",
        "    # Average predictions\n",
        "    predictions /= len(models)\n",
        "    return np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get ensemble predictions\n",
        "y_pred = ensemble_predictions(models, x_test)\n",
        "\n",
        "# Evaluate ensemble performance\n",
        "ensemble_accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
        "print(f'Ensemble Accuracy: {ensemble_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "CUwyaecqbero"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate loss and accuracy curve"
      ],
      "metadata": {
        "id": "gmtRF-fedy9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []  # To store loss and accuracy for each model\n",
        "\n",
        "# Train multiple models in the ensemble\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training model {i+1}\")\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        validation_data=(x_test, y_test),\n",
        "        epochs=10,  # Number of epochs\n",
        "        batch_size=64,\n",
        "        verbose=1,\n",
        "    )\n",
        "    # Append training history for plotting\n",
        "    history.append(hist.history)\n"
      ],
      "metadata": {
        "id": "csna8Su4d7aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine history from all models\n",
        "history_df = pd.DataFrame()\n",
        "for i, hist in enumerate(history):\n",
        "    temp_df = pd.DataFrame(hist)\n",
        "    temp_df[\"Epoch\"] = range(1, len(hist[\"loss\"]) + 1)\n",
        "    temp_df[\"Model\"] = f\"Model {i+1}\"\n",
        "    history_df = pd.concat([history_df, temp_df])\n",
        "\n",
        "melted_df = pd.melt(\n",
        "    history_df,\n",
        "    id_vars=[\"Epoch\", \"Model\"],\n",
        "    value_vars=[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"],\n",
        "    var_name=\"Metric\",\n",
        "    value_name=\"Value\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "jMn9e2qqd_4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the curve\n"
      ],
      "metadata": {
        "id": "OS1jTFn8eSSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme_minimal\n",
        "\n",
        "loss_plot = (\n",
        "    ggplot(melted_df[melted_df[\"Metric\"].isin([\"loss\", \"val_loss\"])], aes(x=\"Epoch\", y=\"Value\", color=\"Metric\"))\n",
        "    + geom_line()\n",
        "    + facet_wrap(\"~Model\")\n",
        "    + labs(\n",
        "        title=\"Training and Validation Loss\",\n",
        "        x=\"Epoch\",\n",
        "        y=\"Loss\",\n",
        "        color=\"Metric\",\n",
        "    )\n",
        "    + theme_minimal()\n",
        ")\n",
        "print(loss_plot)\n"
      ],
      "metadata": {
        "id": "EVCLBUgheQy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_plot = (\n",
        "    ggplot(melted_df[melted_df[\"Metric\"].isin([\"accuracy\", \"val_accuracy\"])], aes(x=\"Epoch\", y=\"Value\", color=\"Metric\"))\n",
        "    + geom_line()\n",
        "    + facet_wrap(\"~Model\")\n",
        "    + labs(\n",
        "        title=\"Training and Validation Accuracy\",\n",
        "        x=\"Epoch\",\n",
        "        y=\"Accuracy\",\n",
        "        color=\"Metric\",\n",
        "    )\n",
        "    + theme_minimal()\n",
        ")\n",
        "print(accuracy_plot)\n"
      ],
      "metadata": {
        "id": "yYktOOdoegZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot confusion matrix"
      ],
      "metadata": {
        "id": "nklwEQQBhvSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"Class {i}\" for i in range(cm.shape[0])],\n",
        "    columns=[f\"Class {i}\" for i in range(cm.shape[1])],\n",
        ")\n",
        "cm_melted = cm_df.reset_index().melt(id_vars=\"index\")\n",
        "cm_melted.columns = [\"True Class\", \"Predicted Class\", \"Count\"]\n",
        "\n",
        "confusion_matrix_plot = (\n",
        "    ggplot(cm_melted, aes(x=\"Predicted Class\", y=\"True Class\", fill=\"Count\"))\n",
        "    + geom_tile(color=\"white\")  # Create the heatmap tiles\n",
        "    + geom_text(aes(label=\"Count\"), size=6, color=\"black\")  # Add text annotations\n",
        "    + scale_fill_gradient(low=\"white\", high=\"blue\")  # Set color gradient\n",
        "    + labs(\n",
        "        title=\"Confusion Matrix\",\n",
        "        x=\"Predicted Class\",\n",
        "        y=\"True Class\",\n",
        "        fill=\"Count\",\n",
        "    )\n",
        "    + theme_minimal()\n",
        ")\n",
        "print(confusion_matrix_plot)\n"
      ],
      "metadata": {
        "id": "i2SQADx9fqXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}